{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def softmax_prob_forloop(W, b, inputs): # output is datalen-by-C (NumPy, no JAX here)\n",
    "    # inputs is dim-by-datalen\n",
    "    # b is C-dimensional vector W is (C-by-dim)\n",
    "    dim, datalen = np.shape(inputs) # how many dimensions, points\n",
    "    c = len(b) # number of classes, C, each class has a bias \n",
    "    score = np.zeros((c, datalen))\n",
    "    for ci in range(c):\n",
    "        for lj in range(datalen):\n",
    "            score[ci, lj] = b[ci]\n",
    "            for dk in range(dim):\n",
    "                score[ci, lj] += W[ci, dk]*inputs[dk, lj]\n",
    "    maxes = np.zeros(datalen)\n",
    "    for lj in range(datalen):\n",
    "        maxes[lj] = np.max(score[:, lj])\n",
    "    for ci in range(c):\n",
    "        for lj in range(datalen):\n",
    "            score[ci, lj] = score[ci, lj] - maxes[lj]\n",
    "    # subtract off the largest score from the bias of each class \n",
    "    # This is for stability to underflow/overflow when exponentiating\n",
    "    expscore = np.exp(score)\n",
    "    norm_factor = np.diag(1/np.sum(expscore, axis=0))\n",
    "    return np.dot(expscore, norm_factor).T  \n",
    "\n",
    "\n",
    "# below we convert the same steps into vector form, hence no for loops\n",
    "\n",
    "def softmax_prob1(W, b, inputs):  # output is datalen-by-C\n",
    "    # inputs is dim-by-datalen\n",
    "    # b is C-dimensional vector W is (C-by-dim)\n",
    "    # Make sure all numerical operations are from JAX, so 'jnp', not 'np'\n",
    "    datalen = jnp.shape(inputs)[1] # how many points\n",
    "    c = len(b) # number of classes, C, each class has a bias \n",
    "    linear_part = jnp.dot(W, inputs) # (C-by-dim)*(dim-by-datalen) = C-by-datalen\n",
    "    large = jnp.max(linear_part, axis=0) # largest of the class scores for each data point\n",
    "    bias_offset = jnp.dot(jnp.diag(b),jnp.ones((c, datalen))) # (C-by-C)*(C-by-L)\n",
    "    # subtract off the largest score from the bias of each class for stability to underflow/overflow\n",
    "    large_offset = jnp.dot(np.ones((c, datalen)),jnp.diag(large)) #  (C-by-L)*(L-by-L)    \n",
    "    expscore = jnp.exp(linear_part + bias_offset - large_offset)\n",
    "    norm_factor = jnp.diag(1/jnp.sum(expscore, axis=0))\n",
    "    return jnp.dot(expscore, norm_factor).T \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}